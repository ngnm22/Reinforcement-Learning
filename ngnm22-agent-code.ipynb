{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TQC:https://github.com/brickerino/ada-tqc/blob/master/tqc/structures.py\n",
    "#ADA:https://github.com/SamsungLabs/tqc_pytorch/blob/master/main.py\n",
    "#ACC:https://github.com/Nicolinho/ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wg1M9-ACbf5g",
    "outputId": "dcb7d4ea-99d6-4189-d7f5-a4ee2045568b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease                        \u001b[0m\u001b[33m\n",
      "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]      \u001b[0m\u001b[33m\n",
      "Err:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]    \u001b[0m\u001b[33m\n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Reading package lists... Done0m0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is not signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.14).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\n",
      "Requirement already satisfied: swig in /opt/conda/lib/python3.7/site-packages (4.1.1)\n",
      "Requirement already satisfied: pyglet==1.5.11 in /opt/conda/lib/python3.7/site-packages (1.5.11)\n",
      "Requirement already satisfied: gym[box2d]==0.20.0 in /opt/conda/lib/python3.7/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.20.0) (1.21.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.20.0) (2.2.1)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.20.0) (1.5.11)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/conda/lib/python3.7/site-packages (from gym[box2d]==0.20.0) (2.3.5)\n",
      "Requirement already satisfied: pyvirtualdisplay==3.0 in /opt/conda/lib/python3.7/site-packages (3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install xvfb -y\n",
    "!apt-get install python-opengl -y\n",
    "!pip install 'swig'\n",
    "!pip install 'pyglet==1.5.11'\n",
    "!pip install 'gym[box2d]==0.20.0'\n",
    "!pip install 'pyvirtualdisplay==3.0'\n",
    "!pip install matplotlib\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "%matplotlib inline\n",
    "#from xvfbwrapper import Xvfb\n",
    "#xvfb-run -s\n",
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "plot_interval = 10 # update the plot every N episodes\n",
    "video_every = 100 # videos can take a very long time to render so only do it every N episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JoiK9kSxMWRM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "%matplotlib inline\n",
    "\n",
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "plot_interval = 10 # update the plot every N episodes\n",
    "video_every = 100 # videos can take a very long time to render so only do it every N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CObYnMaKJFVK"
   },
   "outputs": [],
   "source": [
    "    def states_by_ptr(self, ptr_list, cpu=False):\n",
    "        ind = np.array([], dtype='int64')\n",
    "        for interval in ptr_list:\n",
    "            if interval[0] < interval[1]:\n",
    "                ind = np.concatenate((ind, np.arange(interval[0], interval[1])))\n",
    "            elif interval[0] > interval[1]:\n",
    "                ind = np.concatenate((ind, np.arange(interval[0], self.max_size)))\n",
    "                ind = np.concatenate((ind, np.arange(0, interval[1])))\n",
    "\n",
    "        names = ('state', 'action')\n",
    "        if cpu:\n",
    "            return (torch.FloatTensor(getattr(self, name)[ind]) for name in names)\n",
    "        else:\n",
    "            return (torch.FloatTensor(getattr(self, name)[ind]).to(DEVICE) for name in names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E4yW-EHwMZd_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "DEVICE = device\n",
    "\n",
    "\n",
    "def eval_policy(policy, eval_env, max_episode_steps, eval_episodes=10):\n",
    "    policy.eval()\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        t = 0\n",
    "        while not done and t < max_episode_steps:\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "            t += 1\n",
    "    avg_reward /= eval_episodes\n",
    "    policy.train()\n",
    "    return avg_reward\n",
    "\n",
    "def eval_bias(policy, critic, eval_env, gamma, max_episode_steps, extra_steps, eval_episodes, target_entropy, alpha):\n",
    "    policy.train()\n",
    "    all_values = []\n",
    "    all_returns = []\n",
    "    for _ in range(eval_episodes):\n",
    "        ep_rewards = []\n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        state, done = eval_env.reset(), False\n",
    "        t = 0\n",
    "        while not done and t < max_episode_steps + extra_steps:\n",
    "            action = policy.select_action(state)\n",
    "            ep_states.append(state)\n",
    "            ep_actions.append(action)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "            ep_rewards.append(reward)\n",
    "            t += 1\n",
    "\n",
    "        cur_return = 0\n",
    "        returns = []\n",
    "        for r in ep_rewards[::-1]:\n",
    "            cur_return = r + alpha * target_entropy + cur_return * gamma\n",
    "            returns.append(cur_return)\n",
    "        returns = returns[::-1]\n",
    "\n",
    "        n_to_use = min(max_episode_steps, t)\n",
    "        returns_to_use = np.array(returns[:n_to_use])\n",
    "        all_returns.append(returns_to_use)\n",
    "\n",
    "        states_to_use = np.stack(ep_states[:n_to_use])\n",
    "        states_to_use_torch = torch.FloatTensor(states_to_use).to(DEVICE)\n",
    "        actions_to_use = np.stack(ep_actions[:n_to_use])\n",
    "        actions_to_use_torch = torch.FloatTensor(actions_to_use).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            values = critic(states_to_use_torch, actions_to_use_torch).mean(axis=2).mean(axis=1)\n",
    "            values = values.cpu().numpy()\n",
    "        all_values.append(values)\n",
    "    return all_values, all_returns\n",
    "\n",
    "\n",
    "def quantile_huber_loss_f(quantiles, samples, samples_mask=None):\n",
    "    pairwise_delta = samples[:, None, None, :] - quantiles[:, :, :, None]  # batch x nets x quantiles x samples\n",
    "    abs_pairwise_delta = torch.abs(pairwise_delta)\n",
    "    huber_loss = torch.where(abs_pairwise_delta > 1,\n",
    "                             abs_pairwise_delta - 0.5,\n",
    "                             pairwise_delta ** 2 * 0.5)\n",
    "\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    tau = torch.arange(n_quantiles, device=DEVICE).float() / n_quantiles + 1 / 2 / n_quantiles\n",
    "    loss = (torch.abs(tau[None, None, :, None] - (pairwise_delta < 0).float()) * huber_loss)\n",
    "    if samples_mask is not None:\n",
    "        samples_mask = samples_mask.unsqueeze(0).unsqueeze(0)\n",
    "        loss = (loss * samples_mask).sum(-1) / samples_mask.sum()\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA-fHCauMcUr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oI89-4u5hS4x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Module, Linear\n",
    "from torch.distributions import Distribution, Normal\n",
    "from torch.nn.functional import relu, logsigmoid\n",
    "from gym import spaces\n",
    "import gym\n",
    "from collections import deque\n",
    "from math import pow\n",
    "\n",
    "\n",
    "\n",
    "LOG_STD_MIN_MAX = (-20, 2)\n",
    "\n",
    "\n",
    "class RescaleAction(gym.ActionWrapper):\n",
    "    def __init__(self, env, a, b):\n",
    "        assert isinstance(env.action_space, spaces.Box), (\n",
    "            \"expected Box action space, got {}\".format(type(env.action_space)))\n",
    "        assert np.less_equal(a, b).all(), (a, b)\n",
    "        super(RescaleAction, self).__init__(env)\n",
    "        self.a = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + a\n",
    "        self.b = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + b\n",
    "        self.action_space = spaces.Box(low=a, high=b, shape=env.action_space.shape, dtype=env.action_space.dtype)\n",
    "\n",
    "    def action(self, action):\n",
    "        assert np.all(np.greater_equal(action, self.a)), (action, self.a)\n",
    "        assert np.all(np.less_equal(action, self.b)), (action, self.b)\n",
    "        low = self.env.action_space.low\n",
    "        high = self.env.action_space.high\n",
    "        action = low + (high - low)*((action - self.a)/(self.b - self.a))\n",
    "        action = np.clip(action, low, high)\n",
    "        return action\n",
    "\n",
    "\n",
    "class Mlp(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_sizes,\n",
    "            output_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: initialization\n",
    "        self.fcs = []\n",
    "        in_size = input_size\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            fc = Linear(in_size, next_size)\n",
    "            self.add_module(f'fc{i}', fc)\n",
    "            self.fcs.append(fc)\n",
    "            in_size = next_size\n",
    "        self.last_fc = Linear(in_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = input\n",
    "        for fc in self.fcs:\n",
    "            h = relu(fc(h))\n",
    "        output = self.last_fc(h)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6), gamma=0.99, n_episodes_to_store=50, q_g_rollout_length=None):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes_to_store = n_episodes_to_store\n",
    "        self.q_g_rollout_length = q_g_rollout_length\n",
    "\n",
    "        self.transition_names = ('state', 'action', 'next_state', 'reward', 'not_done', 'ep_end',\n",
    "                                 'returns', 'ep_length', 'bs_multiplier')\n",
    "        sizes = (state_dim, action_dim, state_dim, 1, 1, 1, 1, 1, 1)\n",
    "        for name, size in zip(self.transition_names, sizes):\n",
    "            setattr(self, name, np.empty((max_size, size)))\n",
    "\n",
    "        self.last_episodes = deque()\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done, ep_end):\n",
    "        values = (state, action, next_state, reward, 1. - done, ep_end)\n",
    "        for name, value in zip(self.transition_names, values):\n",
    "            getattr(self, name)[self.ptr] = value\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        if ep_end:\n",
    "            res_idx = []    # indices to put into fresh replay buffer\n",
    "            running_return = 0\n",
    "            running_tail_return = 0\n",
    "            was_timelimit = self.not_done[(self.ptr - 1) % self.max_size, 0] > 0.5\n",
    "            for t in range(self.size):\n",
    "                idx = (self.ptr - 1 - t) % self.max_size   # index of tuple in replay\n",
    "                if t > 0 and (self.ep_end[idx, 0] > 0.5):\n",
    "                    break                                  # got to the previous episode end\n",
    "                running_return = self.reward[idx] + self.gamma * running_return       # running return at current index\n",
    "                if t >= self.q_g_rollout_length:\n",
    "                    running_tail_return = self.reward[(idx + self.q_g_rollout_length) % self.max_size] + self.gamma * running_tail_return\n",
    "                self.returns[idx] = running_return - np.power(self.gamma, self.q_g_rollout_length) * running_tail_return       # only indices in between\n",
    "                self.ep_length[idx] = t + 1\n",
    "\n",
    "                is_enough_rollout = t + 1 > self.q_g_rollout_length\n",
    "                if (was_timelimit and is_enough_rollout) or not was_timelimit:\n",
    "                    res_idx.append(idx)\n",
    "                    if is_enough_rollout:\n",
    "                        self.bs_multiplier[idx] = 1.0\n",
    "                    else:\n",
    "                        self.bs_multiplier[idx] = 0.0\n",
    "            if len(res_idx) > 5:\n",
    "                self.last_episodes.append(np.array(res_idx, dtype='int32'))\n",
    "                if len(self.last_episodes) > self.n_episodes_to_store:\n",
    "                    self.last_episodes.popleft()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        names = self.transition_names[:-2]\n",
    "        return (torch.FloatTensor(getattr(self, name)[ind]).to(DEVICE) for name in names)\n",
    "\n",
    "    def gather_returns(self, gamma, alpha, n_per_episode):\n",
    "        selected_idx = []\n",
    "        for ep_lst in self.last_episodes:\n",
    "            selected_idx.append(np.random.choice(ep_lst, replace=True, size=n_per_episode))\n",
    "        selected_idx = np.concatenate(selected_idx)\n",
    "        return self.get_returns_by_idx(selected_idx, gamma, alpha)\n",
    "\n",
    "    def gather_returns_uniform(self, gamma, alpha, n_per_episode):\n",
    "        all_idx = np.concatenate(self.last_episodes)\n",
    "        selected_idx = np.random.choice(all_idx, replace=True, size=n_per_episode * len(self.last_episodes))\n",
    "        return self.get_returns_by_idx(selected_idx, gamma, alpha)\n",
    "\n",
    "    def get_returns_by_idx(self, selected_idx, gamma, alpha):\n",
    "        h_target = - self.action.shape[1]\n",
    "        result_states = self.state[selected_idx]\n",
    "        result_actions = self.action[selected_idx]\n",
    "        result_bs_multiplier = self.bs_multiplier[selected_idx]\n",
    "        result_bs_states = self.state[(selected_idx + self.q_g_rollout_length) % self.max_size]         # if bs_multiplier is 0 non-existing states should not matter\n",
    "        ep_length = self.ep_length[selected_idx]\n",
    "        ns = (ep_length - 1) * (1 - result_bs_multiplier) + self.q_g_rollout_length * result_bs_multiplier\n",
    "        entropy_сorrection = alpha * h_target * gamma * (1 - np.power(gamma, ns)) / (1 - gamma)\n",
    "        result_returns = self.returns[selected_idx] + entropy_сorrection\n",
    "        return (torch.tensor(arr, dtype=torch.float32, device=DEVICE) for arr in\n",
    "                [result_states, result_actions, result_returns, result_bs_states, result_bs_multiplier])\n",
    "\n",
    "    def states_by_ptr(self, ptr_list, cpu=False):\n",
    "        ind = np.array([], dtype='int64')\n",
    "        for interval in ptr_list:\n",
    "            if interval[0] < interval[1]:\n",
    "                ind = np.concatenate((ind, np.arange(interval[0], interval[1])))\n",
    "            elif interval[0] > interval[1]:\n",
    "                ind = np.concatenate((ind, np.arange(interval[0], self.max_size)))\n",
    "                ind = np.concatenate((ind, np.arange(0, interval[1])))\n",
    "\n",
    "        names = ('state', 'action')\n",
    "        return (torch.FloatTensor(getattr(self, name)[ind]).to(DEVICE) for name in names)\n",
    "\n",
    "class Critic(Module):\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets):\n",
    "        super().__init__()\n",
    "        self.nets = []\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.n_nets = n_nets\n",
    "        for i in range(n_nets):\n",
    "            net = Mlp(state_dim + action_dim, [512, 512, 512], n_quantiles)\n",
    "            self.add_module(f'qf{i}', net)\n",
    "            self.nets.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat((state, action), dim=1)\n",
    "        quantiles = torch.stack(tuple(net(sa) for net in self.nets), dim=1)\n",
    "        return quantiles\n",
    "\n",
    "\n",
    "class Agent(Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.net = Mlp(state_dim, [256, 256], 2 * action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.net(obs).split([self.action_dim, self.action_dim], dim=1)\n",
    "        log_std = log_std.clamp(*LOG_STD_MIN_MAX)\n",
    "\n",
    "        if self.training:\n",
    "            std = torch.exp(log_std)\n",
    "            tanh_normal = TanhNormal(mean, std)\n",
    "            action, pre_tanh = tanh_normal.rsample()\n",
    "            log_prob = tanh_normal.log_prob(pre_tanh)\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        else:  # deterministic eval without log_prob computation\n",
    "            action = torch.tanh(mean)\n",
    "            log_prob = None\n",
    "        return action, log_prob\n",
    "\n",
    "    def sample_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs).to(DEVICE)[None, :]\n",
    "        action, _ = self.forward(obs)\n",
    "        action = action[0].cpu().detach().numpy()\n",
    "        return action\n",
    "\n",
    "\n",
    "class TanhNormal(Distribution):\n",
    "    def __init__(self, normal_mean, normal_std):\n",
    "        super().__init__()\n",
    "        self.normal_mean = normal_mean\n",
    "        self.normal_std = normal_std\n",
    "        self.standard_normal = Normal(torch.zeros_like(self.normal_mean, device=DEVICE),\n",
    "                                      torch.ones_like(self.normal_std, device=DEVICE))\n",
    "        self.normal = Normal(normal_mean, normal_std)\n",
    "\n",
    "    def log_prob(self, pre_tanh):\n",
    "        log_det = 2 * np.log(2) + logsigmoid(2 * pre_tanh) + logsigmoid(-2 * pre_tanh)\n",
    "        result = self.normal.log_prob(pre_tanh) - log_det\n",
    "        return result\n",
    "\n",
    "    def rsample(self):\n",
    "        pretanh = self.normal_mean + self.normal_std * self.standard_normal.sample()\n",
    "        return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAvZlpmwMe9G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4POKfNvbhZzr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import ceil, floor\n",
    "\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_target,\n",
    "        discount,\n",
    "        tau,\n",
    "        top_quantiles_to_drop,\n",
    "        target_entropy,\n",
    "        sampling_scheme,\n",
    "        Q_G_eval_interval,\n",
    "        Q_G_n_per_episode,\n",
    "        delta_gamma,\n",
    "        d_update_interval,use_acc,\n",
    "                      lr_dropped_quantiles,\n",
    "                      adjusted_dropped_quantiles_init,\n",
    "                      adjusted_dropped_quantiles_max,\n",
    "                      diff_ma_coef,\n",
    "                      num_critic_updates\n",
    "    ):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.critic_target = critic_target\n",
    "        self.log_alpha = torch.zeros((1,), requires_grad=True, device=DEVICE)\n",
    "\n",
    "        # TODO: check hyperparams\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=3e-4)\n",
    "\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.top_quantiles_to_drop = top_quantiles_to_drop\n",
    "        self.target_entropy = target_entropy\n",
    "\n",
    "        self.quantiles_total = critic.n_quantiles * critic.n_nets\n",
    "        self.samples_mask = torch.zeros(1, self.quantiles_total, dtype=torch.float32, device=DEVICE)\n",
    "        self.calculate_quantile_mask_2()\n",
    "\n",
    "        self.total_it = 0\n",
    "        self.Q_G_eval_interval = Q_G_eval_interval\n",
    "        self.Q_G_n_per_episode = Q_G_n_per_episode\n",
    "        self.Q_G_delta = 0\n",
    "        self.sampling_scheme = sampling_scheme\n",
    "        self.delta_gamma = delta_gamma\n",
    "        self.d_update_interval = d_update_interval\n",
    "        self.use_acc = use_acc\n",
    "        self.num_critic_updates = num_critic_updates\n",
    "        if use_acc:\n",
    "            self.adjusted_dropped_quantiles = torch.tensor(adjusted_dropped_quantiles_init, requires_grad=True)\n",
    "            self.adjusted_dropped_quantiles_max = adjusted_dropped_quantiles_max\n",
    "            self.dropped_quantiles_dropped_optimizer = torch.optim.SGD([self.adjusted_dropped_quantiles], lr=lr_dropped_quantiles)\n",
    "            self.first_training = True\n",
    "            self.diff_ma_coef = diff_ma_coef\n",
    "\n",
    "    def calculate_quantile_mask(self):\n",
    "        top_quantiles_to_drop = self.log_eta.sigmoid() * self.quantiles_total\n",
    "        self.samples_mask = torch.zeros(1, self.quantiles_total, dtype=torch.float64, device=DEVICE)\n",
    "        for i in range(self.quantiles_total):\n",
    "            mask = self.quantiles_total - top_quantiles_to_drop - i\n",
    "            self.samples_mask[0, i] = max(min(mask, 1), 0)\n",
    "\n",
    "    def calculate_quantile_mask_2(self):\n",
    "        top_quantiles_to_drop = self.top_quantiles_to_drop\n",
    "        top = ceil(top_quantiles_to_drop)\n",
    "        bot = floor(top_quantiles_to_drop)\n",
    "        self.samples_mask[0, 0:self.quantiles_total - top] = 1\n",
    "        self.samples_mask[0, self.quantiles_total - bot:] = 0\n",
    "        if top != bot:\n",
    "            self.samples_mask[0, self.quantiles_total - top] = top - top_quantiles_to_drop\n",
    "\n",
    "    def add_next_z_metrics(self, metrics, next_z):\n",
    "        for t in range(1, self.critic.n_quantiles + 1):\n",
    "            total_quantiles_to_keep = t * self.critic.n_nets\n",
    "            metrics[f'Target_Q/Q_value_t={t}'] = next_z[:, :total_quantiles_to_keep].mean().__float__()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        metrics = dict()\n",
    "        #alpha = torch.exp(self.log_alpha)\n",
    "        if ptr_list is not None and do_beta_update:\n",
    "            self.update_beta(replay_buffer, ptr_list, disc_return)\n",
    "\n",
    "        for it in range(self.num_critic_updates):\n",
    "            state, action, next_state, reward, not_done, *_ = replay_buffer.sample(batch_size)\n",
    "            alpha = torch.exp(self.log_alpha)\n",
    "            metrics['alpha'] = alpha.item()\n",
    "            metrics['top_quantiles_to_drop'] = self.top_quantiles_to_drop\n",
    "\n",
    "\n",
    "            # --- Q loss ---\n",
    "            with torch.no_grad():\n",
    "                # get policy action\n",
    "                new_next_action, next_log_pi = self.actor(next_state)\n",
    "                # compute and cut quantiles at the next state\n",
    "                next_z = self.critic_target(next_state, new_next_action)  # batch x nets x quantiles\n",
    "                sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "                self.add_next_z_metrics(metrics, sorted_z)\n",
    "                if self.use_acc:\n",
    "                    sorted_z_part = sorted_z[:, :self.quantiles_total]\n",
    "                else:\n",
    "                    sorted_z_part = sorted_z[:, :self.quantiles_total]\n",
    "\n",
    "                # compute target\n",
    "\n",
    "                target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "\n",
    "        cur_z = self.critic(state, action)\n",
    "        critic_loss = quantile_huber_loss_f(cur_z, target, self.samples_mask)\n",
    "        metrics['critic_loss'] = critic_loss.item()\n",
    "\n",
    "        # --- Policy and alpha loss ---\n",
    "        new_action, log_pi = self.actor(state)\n",
    "        metrics['actor_entropy'] = - log_pi.mean().item()\n",
    "        alpha_loss = -self.log_alpha * (log_pi + self.target_entropy).detach().mean()\n",
    "\n",
    "\n",
    "        # --- Update ---\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "        metrics['actor_loss'] = actor_loss.item()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        if self.total_it > 10000 and (self.total_it + 1) % self.Q_G_eval_interval == 0:\n",
    "            self.eval_thresholds(replay_buffer, self.Q_G_n_per_episode)\n",
    "\n",
    "        if self.total_it > 10000 and (self.total_it + 1) % self.d_update_interval == 0:\n",
    "            self.update_d()\n",
    "\n",
    "        self.total_it += 1\n",
    "        return metrics\n",
    "    def update_beta(self, replay_buffer, ptr_list=None, disc_return=None):\n",
    "            state, action = replay_buffer.states_by_ptr(ptr_list)\n",
    "            disc_return = torch.FloatTensor(disc_return).to(DEVICE)\n",
    "            assert disc_return.shape[0] == state.shape[0]\n",
    "\n",
    "            mean_Q_last_eps =  self.critic(state, action).mean(2).mean(1, keepdim=True).mean().detach()\n",
    "            mean_return_last_eps = torch.mean(disc_return).detach()\n",
    "\n",
    "            if self.first_training:\n",
    "                    self.diff_mvavg = torch.abs(mean_return_last_eps - mean_Q_last_eps).detach()\n",
    "                    self.first_training = False\n",
    "            else:\n",
    "                    self.diff_mvavg = (1 - self.diff_ma_coef) * self.diff_mvavg \\\n",
    "                                                        + self.diff_ma_coef * torch.abs(mean_return_last_eps - mean_Q_last_eps).detach()\n",
    "\n",
    "            diff_qret = ((mean_return_last_eps - mean_Q_last_eps) / (self.diff_mvavg + 1e-8)).detach()\n",
    "            aux_loss = self.adjusted_dropped_quantiles * diff_qret\n",
    "            self.dropped_quantiles_dropped_optimizer.zero_grad()\n",
    "            aux_loss.backward()\n",
    "            self.dropped_quantiles_dropped_optimizer.step()\n",
    "            self.adjusted_dropped_quantiles.data = self.adjusted_dropped_quantiles.clamp(min=0., max=self.adjusted_dropped_quantiles_max)\n",
    "\n",
    "    def eval_thresholds_by_type(self, replay_buffer, n_per_episode, sampling_scheme):\n",
    "        res = dict()\n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "        if sampling_scheme == 'uniform':\n",
    "            states, actions, returns, bs_states, bs_multiplier = replay_buffer.gather_returns_uniform(self.discount, float(alpha), n_per_episode)\n",
    "        elif sampling_scheme == 'episodes':\n",
    "            states, actions, returns, bs_states, bs_multiplier = replay_buffer.gather_returns(self.discount, float(alpha), n_per_episode)\n",
    "        else:\n",
    "            raise Exception(\"No such sampling scheme\")\n",
    "        tail_actions = self.actor(bs_states)[0]\n",
    "        tail_z = self.critic_target(bs_states, tail_actions)\n",
    "        tail_z = tail_z.reshape(tail_z.shape[0], -1)\n",
    "        tail_z = tail_z.mean(1, keepdim=True) * bs_multiplier * np.power(replay_buffer.gamma, replay_buffer.q_g_rollout_length)\n",
    "        res[f'LastReplay_{sampling_scheme}/Returns'] = (returns + tail_z).mean().__float__()\n",
    "\n",
    "        cur_z = self.critic(states, actions)\n",
    "        cur_z = cur_z.reshape(cur_z.shape[0], -1)\n",
    "        cur_z = cur_z.sort(dim=1)[0]\n",
    "        for t in range(1, self.critic.n_quantiles + 1):\n",
    "            total_quantiles_to_keep = t * self.critic.n_nets\n",
    "            res[f'LastReplay_{sampling_scheme}/Q_value_t={t}'] = cur_z[:, :total_quantiles_to_keep].mean().__float__()\n",
    "        return res\n",
    "\n",
    "    def eval_thresholds(self, replay_buffer, n_per_episode):\n",
    "        res_uniform = self.eval_thresholds_by_type(replay_buffer, n_per_episode, 'uniform')\n",
    "        res_episodes = self.eval_thresholds_by_type(replay_buffer, n_per_episode, 'episodes')\n",
    "        res = dict()\n",
    "        res.update(res_uniform)\n",
    "        res.update(res_episodes)\n",
    "        last_Q_G_delta = res[f'LastReplay_{self.sampling_scheme}/Q_value_t={self.critic.n_quantiles}'] - \\\n",
    "                         res[f'LastReplay_{self.sampling_scheme}/Returns']\n",
    "        self.Q_G_delta = self.Q_G_delta * self.delta_gamma + last_Q_G_delta * (1 - self.delta_gamma)\n",
    "        return res\n",
    "\n",
    "    def update_d(self):\n",
    "        if self.Q_G_delta < 0 and self.top_quantiles_to_drop > 0:\n",
    "            self.top_quantiles_to_drop -= 1\n",
    "        elif self.Q_G_delta > 0 and self.top_quantiles_to_drop < self.quantiles_total:\n",
    "            self.top_quantiles_to_drop += 1\n",
    "        self.calculate_quantile_mask_2()\n",
    "    def save(self, filename):\n",
    "        filename = str(filename)\n",
    "        self.light_save(filename)\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "        torch.save(self.alpha_optimizer.state_dict(), filename + \"_alpha_optimizer\")\n",
    "\n",
    "    def light_save(self, filename):\n",
    "        filename = str(filename)\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_target.state_dict(), filename + \"_critic_target\")\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.log_alpha, filename + \"_log_alpha\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        filename = str(filename)\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_target.load_state_dict(torch.load(filename + \"_critic_target\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.log_alpha = torch.load(filename + '_log_alpha')\n",
    "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=3e-4)\n",
    "        self.alpha_optimizer.load_state_dict(torch.load(filename + \"_alpha_optimizer\"))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#env = gym.make(\"BipedalWalker-v3\")\n",
    "#eval_env = gym.make(\"BipedalWalker-v3\")\n",
    "env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
    "eval_env = gym.make(\"BipedalWalkerHardcore-v3\") \n",
    "env = gym.wrappers.Monitor(env, \"/workspace/video5\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FUw4h980jfnu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment has 24 observations and the agent can take 4 actions\n",
      "The device is: cuda\n",
      "It's recommended to train on the cpu for this\n"
     ]
    }
   ],
   "source": [
    "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
    "print('The device is: {}'.format(device))\n",
    "\n",
    "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vEmysJsYZ0nx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pybullet_envs\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for pybullet_envs\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDl6ViIDlVOk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 760, reward: 67.77401273602067\n",
      "\n",
      "episode: 761, reward: 112.29566233904774\n",
      "\n",
      "episode: 762, reward: -54.795982101625455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "2# in the submission please use seed 42 for verification\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "import copy\n",
    "import datetime\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "# logging variables\n",
    "ep_reward = 0\n",
    "reward_list = []\n",
    "plot_data = []\n",
    "log_f = open(\"/workspace/agent-log23.txt\",\"w+\")\n",
    "n_quantiles = 25\n",
    "top_quantiles_to_drop_per_net = 1\n",
    "n_nets = 5\n",
    "batch_size = 256\n",
    "discount = 0.99\n",
    "tau = 0.005\n",
    "# initialise agent\n",
    "Q_G_eval_interval = 50\n",
    "Q_G_n_episodes = 50\n",
    "Q_G_n_per_episode =20\n",
    "Q_G_rollout_length = 500\n",
    "delta_gamma = 0.99\n",
    "max_episodes = 200000000\n",
    "max_timesteps = 2000\n",
    "d_update_interval = 50000\n",
    "sampling_scheme = 'episodes'\n",
    "use_acc = True\n",
    "import pickle\n",
    "env = RescaleAction(env, -1., 1.)\n",
    "eval_env = RescaleAction(eval_env, -1., 1.)\n",
    "\n",
    "\n",
    "replay_buffer = ReplayBuffer(obs_dim, act_dim, gamma=discount,\n",
    "                                            n_episodes_to_store=Q_G_n_episodes, q_g_rollout_length=Q_G_rollout_length)\n",
    "agent = Agent(obs_dim, act_dim).to(device)\n",
    "critic = Critic(obs_dim, act_dim, n_quantiles, n_nets).to(device)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "top_quantiles_to_drop = top_quantiles_to_drop_per_net * n_nets\n",
    "\n",
    "trainer = Trainer(actor=agent,\n",
    "                      critic=critic,\n",
    "                      critic_target=critic_target,\n",
    "                      top_quantiles_to_drop=top_quantiles_to_drop,\n",
    "                      discount=discount,\n",
    "                      tau= tau,\n",
    "                      target_entropy=-np.prod(env.action_space.shape).item(),sampling_scheme=sampling_scheme,\n",
    "                      Q_G_eval_interval=Q_G_eval_interval,\n",
    "                      Q_G_n_per_episode=Q_G_n_per_episode,\n",
    "                      delta_gamma=delta_gamma,\n",
    "                      d_update_interval=d_update_interval,                      \n",
    "                      use_acc=use_acc,\n",
    "                      lr_dropped_quantiles=0.1,\n",
    "                      adjusted_dropped_quantiles_init=2.5,\n",
    "                      adjusted_dropped_quantiles_max=5.0,\n",
    "                      diff_ma_coef=0.05,\n",
    "                      num_critic_updates=1)\n",
    "\n",
    "evaluations = []\n",
    "state, done = env.reset(), False\n",
    "\n",
    "total_num_steps = 0\n",
    "episode_return, last_episode_return = 0, 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "if use_acc:\n",
    "    reward_list = []\n",
    "    start_ptr = replay_buffer.ptr\n",
    "    ptr_list = []\n",
    "    disc_return = []\n",
    "    time_since_beta_update = 0\n",
    "    do_beta_update = False\n",
    "agent.train()\n",
    "beta_udate_rate = 1000\n",
    "init_num_steps = 25000\n",
    "size_limit_beta_update_batch = 5000\n",
    "reward_list2 = []\n",
    "# training procedure:\n",
    "for episode in range(1, max_episodes+1):\n",
    "    for t in range(max_timesteps):\n",
    "        #print(t)\n",
    "        total_num_steps += 1\n",
    "        episode_timesteps += 1\n",
    "        # select the agent action\n",
    "        action = agent.sample_action(state)\n",
    "\n",
    "        # take action in environment and get r and s'\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ep_end = done or total_num_steps >= max_timesteps\n",
    "        replay_buffer.add(state, action, next_state, reward, done, ep_end)\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        # stop iterating when the episode finished\n",
    "        if done or t==(max_timesteps-1):\n",
    "            break\n",
    "        if use_acc:\n",
    "            reward_list2.append(ep_reward)\n",
    "            time_since_beta_update += 1\n",
    "        if done or episode_timesteps >= max_episodes:\n",
    "            if use_acc:\n",
    "                ptr_list.append([start_ptr, replay_buffer.ptr])\n",
    "                start_ptr = replay_buffer.ptr\n",
    "                if t > 1:\n",
    "                    for i in range(episode_timesteps):\n",
    "                        disc_return.append(\n",
    "                            np.sum(np.array(reward_list2)[i:] * (discount ** np.arange(0, episode_timesteps - i))))\n",
    "                    if time_since_beta_update >= beta_udate_rate and t >= init_num_steps:\n",
    "                        do_beta_update = True\n",
    "            reward_list2 = []\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= 10:\n",
    "            if use_acc and do_beta_update:\n",
    "                trainer.train(replay_buffer, batch_size, ptr_list, disc_return, do_beta_update)\n",
    "                do_beta_update= False\n",
    "                for ii, ptr_pair in enumerate(copy.deepcopy(ptr_list)):\n",
    "                    if (ptr_pair[0] < replay_buffer.ptr - size_limit_beta_update_batch):\n",
    "                        disc_return = disc_return[ptr_pair[1] - ptr_pair[0]:]\n",
    "                        ptr_list.pop(0)\n",
    "                    elif (ptr_pair[0] > replay_buffer.ptr and\n",
    "                             replay_buffer.max_size - ptr_pair[0] + replay_buffer.ptr > size_limit_beta_update_batch):\n",
    "                        if ptr_pair[1] > ptr_pair[0]:\n",
    "                            disc_return = disc_return[ptr_pair[1] - ptr_pair[0]:]\n",
    "                        else:\n",
    "                            disc_return = disc_return[replay_buffer.max_size - ptr_pair[0] + ptr_pair[1]:]\n",
    "                        ptr_list.pop(0)\n",
    "                    else:\n",
    "                        break\n",
    "                time_since_beta_update = 0\n",
    "            else:\n",
    "                trainer.train(replay_buffer, batch_size)\n",
    "\n",
    "        if done or episode_timesteps >= max_episodes:\n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(f\"Seed: {seed} Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {ep_reward:.1f}\")\n",
    "            # Reset environment\n",
    "            state, done = env.reset(), False\n",
    "\n",
    "            last_episode_return = episode_return\n",
    "            episode_return = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1  \n",
    "    # append the episode reward to the reward list\n",
    "    reward_list.append(ep_reward)\n",
    "    print('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
    "    # do NOT change this logging code - it is used for automated marking!\n",
    "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
    "    log_f.flush()\n",
    "    ep_reward = 0\n",
    "    state, done = env.reset(), False\n",
    "    episode += 1    \n",
    "    # print reward data every so often - add a graph like this in your report\n",
    "    if episode % plot_interval == 0:\n",
    "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
    "        reward_list = []\n",
    "        reward_list2 = []\n",
    "        # plt.rcParams['figure.dpi'] = 100\n",
    "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.ylabel('Episode reward')\n",
    "        plt.show()\n",
    "        disp.clear_output(wait=True)\n",
    " \n",
    "    if episode%50 == 0:\n",
    "        trainer_save_name = f'/workspace/iterhard3_{episode}'\n",
    "        trainer.save(trainer_save_name)\n",
    "        with open(f'/workspace/iterhard3_{episode}_replay', 'wb') as outF:\n",
    "            pickle.dump(replay_buffer, outF)\n",
    "        with open(f'/workspace/plothard3_{episode}_replay', 'wb') as outF:\n",
    "            pickle.dump(plot_data, outF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "trainer = Trainer(actor=agent,\n",
    "                      critic=critic,\n",
    "                      critic_target=critic_target,\n",
    "                      top_quantiles_to_drop=top_quantiles_to_drop,\n",
    "                      discount=discount,\n",
    "                      tau= tau,\n",
    "                      target_entropy=-np.prod(env.action_space.shape).item(),sampling_scheme=sampling_scheme,\n",
    "                      Q_G_eval_interval=Q_G_eval_interval,\n",
    "                      Q_G_n_per_episode=Q_G_n_per_episode,\n",
    "                      delta_gamma=delta_gamma,\n",
    "                      d_update_interval=d_update_interval,                      \n",
    "                      use_acc=use_acc,\n",
    "                      lr_dropped_quantiles=0.1,\n",
    "                      adjusted_dropped_quantiles_init=2.5,\n",
    "                      adjusted_dropped_quantiles_max=5.0,\n",
    "                      diff_ma_coef=0.05,\n",
    "                      num_critic_updates=1)\n",
    "trainer_save_name = f'/workspace/iterhard3_{episode}'\n",
    "trainer.save(trainer_save_name)\n",
    "with open(f'/workspace/iterhard3_{episode}_replay', 'wb') as outF:\n",
    "    pickle.dump(replay_buffer, outF)\n",
    "with open(f'/workspace/plothard3_{episode}_replay', 'wb') as outF:\n",
    "    pickle.dump(plot_data, outF)  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
